import dataclasses
import os
import re
import time
import warnings
from typing import Any, Dict, List, Tuple

import loguru
from langchain_ollama import OllamaLLM
from tenacity import *

from sspentestlab.utils.APIs.module_import import LangOllamaConfigClass
from sspentestlab.utils.llm_api import LLMAPI
from sspentestlab.utils.APIs.rag_utils import LlamaIndexRAG

warnings.filterwarnings("ignore")

from rich.console import Console
console = Console()

logger = loguru.logger
logger.remove()

@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None

@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id

class OLLAMAAPI(LLMAPI):
    def __init__(self, config_class:LangOllamaConfigClass, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.history_length = 5  # maintain a history of 5 messages
        self.conversation_dict = {}
        self.model = OllamaLLM(model=self.name, num_ctx=4096, num_predict=512, temperature=0.8) # -> main model

        # configure RAG storage
        self.document_dir = "./RAG"
        self.persist_dir = "./storage"
        self.rag_system = (self.document_dir, self.persist_dir)
        
        self.retrieved_info = "" 

    def _combine_rag_with_prompt(self, latest_message: str, retrieved_info: str, temperature: 0.5) -> str:
        """Combine RAG retrieved info with prompt."""
        
        if retrieved_info:
            # if retrieved info is available, combine it with the prompt
            combined_prompt = f"{latest_message}\n\nInformation from documents:\n{retrieved_info}"
        else:
            # if no retrieved info, use the prompt as is
            combined_prompt = latest_message
        return combined_prompt

    def _chat_completion_fallback(self, history: List) -> str:
        """Fallback incase of error."""
        
        try:
            if not history:
                raise ValueError("History is empty, cannot generate a response.")

            logger.debug(f"Fallback prompt: {history[-1]['content']}")
            response = self.model(prompt=history[-1]["content"], top_k=self.history_length)
            logger.debug(f"Fallback response: {response}")
            return response
        except Exception as e:
            logger.error(f"Error in fallback: {e}")
            return "An error occurred in fallback."

    def _chat_completion(self, history: List) -> str:
        try:
            if not history:
                raise ValueError("History is empty, cannot generate a response.")

            # get last registered in history
            latest_message = history[-1]["content"]
            logger.debug(f"Prompt: {latest_message}")

            # get relevant RAG information
            retrieved_info = self.rag_system.query_index(latest_message)
            self.retrieved_info = retrieved_info

            # combine rag with prompt
            combined_prompt = self._combine_rag_with_prompt(latest_message, retrieved_info)
            # debugging
            # print(f"Recovered RAG data: {retrieved_info}")
            # print(f"Combined prompt: {combined_prompt}")
            console.print(f"RAG -> \n {retrieved_info}", style="red")
            # generate the answer
            response = self.model.prompt(prompt=combined_prompt, top_k=self.history_length)
            logger.debug(f"Response: {response}")
            return response
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            return self._chat_completion_fallback(history)