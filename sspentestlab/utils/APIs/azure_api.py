import dataclasses
import os
import time
from typing import Any, Dict, List

import loguru
import requests
import warnings

from sspentestlab.utils.llm_api import LLMAPI

logger = loguru.logger
logger.remove()
logger.add(sink=os.path.join("logs", "chatgpt.log"), level="WARNING")

warnings.filterwarnings("ignore")

@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None


@dataclasses.dataclass
class Conversation:
    conversation_id: str
    history: List[Dict[str, str]] = dataclasses.field(default_factory=list)

    def add_message(self, role: str, content: str):
        self.history.append({"role": role, "content": content})

    def get_history(self):
        return self.history

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id

    def __hash__(self):
        return hash(self.conversation_id)


class AzureGPTAPI(LLMAPI):
    def __init__(self, config_class, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.api_key = config_class.api_key
        self.azure_endpoint = config_class.azure_endpoint.rstrip('/')
        self.api_version = config_class.api_version
        self.deployment_name = config_class.deployment_name
        self.log_dir = config_class.log_dir
        self.history_length = 5  # maintain 5 messages in the history
        self.conversation_dict: Dict[str, Conversation] = {}
        self.error_waiting_time = 5  # wait for 5 seconds

        logger.add(sink=os.path.join(self.log_dir, "chatgpt.log"), level="WARNING")

    def send_message(self, message: str, session_id: str) -> str:
        """Send message to specific conversation."""
        
        if session_id not in self.conversation_dict:
            self.conversation_dict[session_id] = Conversation(session_id)

        conversation = self.conversation_dict[session_id]
        conversation.add_message("user", message)

        url = f"{self.azure_endpoint}/openai/deployments/{self.deployment_name}/chat/completions?api-version={self.api_version}"
        headers = {
            "Content-Type": "application/json",
            "api-key": self.api_key,
        }
        payload = {
            "messages": conversation.get_history(),
            "temperature": 0.5,
        }

        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            response_json = response.json()
            assistant_message = response_json['choices'][0]['message']['content']
            conversation.add_message("assistant", assistant_message)
            return assistant_message
        except requests.exceptions.RequestException as e:
            logger.warning(
                "API Connection Error. Waiting for {} seconds".format(
                    self.error_waiting_time
                )
            )
            logger.error("Connection Error: {}".format(e))
            print(f"Connection Error: {str(e)}")
            time.sleep(self.error_waiting_time)
            try:
                response = requests.post(url, headers=headers, json=payload)
                response.raise_for_status()
                response_json = response.json()
                assistant_message = response_json['choices'][0]['message']['content']
                conversation.add_message("assistant", assistant_message)
                return assistant_message
            except requests.exceptions.RequestException as e:
                logger.error("Request Error: {}".format(e))
                print(f"Request Error: {str(e)}")
                raise
        except requests.exceptions.HTTPError as e:
            logger.warning("HTTP Error. Waiting for 5 seconds")
            logger.error("HTTP Error: {}".format(e))
            print(f"HTTP Error: {str(e)}")
            time.sleep(5)
            try:
                response = requests.post(url, headers=headers, json=payload)
                response.raise_for_status()
                response_json = response.json()
                assistant_message = response_json['choices'][0]['message']['content']
                conversation.add_message("assistant", assistant_message)
                return assistant_message
            except requests.exceptions.RequestException as e:
                logger.error("Request Error: {}".format(e))
                print(f"Request Error: {str(e)}")
                raise


        # if the response is a tuple, it means that the response is not valid.
        if isinstance(response, tuple):
            logger.warning("Response is not valid. Waiting for 5 seconds")
            print("Response is not valid. Waiting for 5 seconds")
            try:
                time.sleep(5)
                response = requests.post(url, headers=headers, json=payload)
                response.raise_for_status()
                response_json = response.json()
                print(f"Response: {response_json}")
                return response_json
            except requests.exceptions.RequestException as e:
                logger.error("Request Error: {}".format(e))
                print(f"Request Error: {str(e)}")
                raise



####################### TEST #######################

# import os
# import time
# import requests
# import dataclasses
# from typing import List, Dict
# from loguru import logger

# from sspentestlab.utils.llm_api import LLMAPI

# @dataclasses.dataclass
# class Conversation:
#     conversation_id: str
#     history: List[Dict[str, str]] = dataclasses.field(default_factory=list)

#     def add_message(self, role: str, content: str):
#         self.history.append({"role": role, "content": content})

#     def get_history(self):
#         return self.history

#     def __eq__(self, other):
#         if not isinstance(other, Conversation):
#             return False
#         return self.conversation_id == other.conversation_id

#     def __hash__(self):
#         return hash(self.conversation_id)


# class AzureGPTAPI(LLMAPI):
#     def __init__(self, config_class, use_langfuse_logging=False):
#         self.name = str(config_class.model)
#         self.api_key = config_class.api_key
#         self.azure_endpoint = config_class.azure_endpoint.rstrip('/')
#         self.api_version = config_class.api_version
#         self.deployment_name = config_class.deployment_name
#         self.log_dir = config_class.log_dir
#         self.history_length = 5  # maintain 5 messages in the history. (5 chat memory)
#         self.conversation_dict: Dict[str, Conversation] = {}
#         self.error_waiting_time = 3  # wait for 3 seconds

#         logger.add(sink=os.path.join(self.log_dir, "chatgpt.log"), level="WARNING")

#     def _chat_completion(
#         self, history: List, model="gpt-3.5-turbo-16k", temperature=0.5
#     ) -> str:
#         url = f"{self.azure_endpoint}/openai/deployments/{self.deployment_name}/chat/completions?api-version={self.api_version}"
#         headers = {
#             "Content-Type": "application/json",
#             "api-key": self.api_key,
#         }
#         payload = {
#             "model": model,
#             "messages": history,
#             "temperature": temperature,
#         }
#         try:
#             response = requests.post(url, headers=headers, json=payload)
#             response.raise_for_status()
#             print(f"Response: {response.json()}")
#             return response.json()
#         except requests.exceptions.RequestException as e:
#             logger.warning(
#                 "API Connection Error. Waiting for {} seconds".format(
#                     self.error_waiting_time
#                 )
#             )
#             logger.error("Connection Error: {}".format(e))
#             print(f"Connection Error: {str(e)}")
#             time.sleep(self.error_waiting_time)
#             response = requests.post(url, headers=headers, json=payload)
#             response.raise_for_status()
#             return response.json()
#         except requests.exceptions.HTTPError as e:
#             logger.warning("HTTP Error. Waiting for 5 seconds")
#             logger.error("HTTP Error: {}".format(e))
#             print(f"HTTP Error: {str(e)}")
#             time.sleep(5)
#             response = requests.post(url, headers=headers, json=payload)
#             response.raise_for_status()
#             return response.json()
#         except requests.exceptions.RequestException as e:
#             logger.error("Request Error: {}".format(e))
#             print(f"Request Error: {str(e)}")
#             raise

#         # if the response is a tuple, it means that the response is not valid.
#         if isinstance(response, tuple):
#             logger.warning("Response is not valid. Waiting for 5 seconds")
#             print("Response is not valid. Waiting for 5 seconds")
#             try:
#                 time.sleep(5)
#                 response = requests.post(url, headers=headers, json=payload)
#                 response.raise_for_status()
#                 return response.json()
#             except requests.exceptions.RequestException as e:
#                 logger.error("Request Error: {}".format(e))
#                 print(f"Request Error: {str(e)}")
#                 raise