import dataclasses
import os
import re
import time
from datetime import datetime
from typing import Any, Dict, List, Tuple

import loguru
import openai
from langfuse import Langfuse
from langfuse.model import InitialGeneration, Usage
from openai import OpenAI

from sspentestlab.utils.llm_api import LLMAPI

logger = loguru.logger
logger.remove()
# logger.add(level="WARNING", sink="logs/chatgpt.log")



@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None



@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id



class ChatGPTAPI(LLMAPI):
    def __init__(self, config_class, use_langfuse_logging=False):
        """Declare langfuse PUBLIC and SECRET keys to be able to track costs"""
        
        self.name = str(config_class.model)
        api_key = os.getenv("OPENAI_API_KEY", None)
        self.client = OpenAI(api_key=api_key, base_url=config_class.api_base)
        
        if use_langfuse_logging:
            # use langfuse.openai to shadow the default openai library
            os.environ["LANGFUSE_PUBLIC_KEY"] = ("pk-lf-b3796db6-6381-4e2a-8965-90972cd3bd16")
            os.environ["LANGFUSE_SECRET_KEY"] = ("sk-lf-287f9b4b-6aad-4739-8de8-1bf17724ff14")

            self.langfuse = Langfuse()

        self.model = config_class.model
        self.log_dir = config_class.log_dir
        self.history_length = 5  # maintain 5 messages in the history. (5 chat memory)
        self.conversation_dict: Dict[str, Conversation] = {}
        self.error_wait_time = config_class.error_wait_time
        logger.add(sink=os.path.join(self.log_dir, "chatgpt.log"), level="WARNING")



    def _chat_completion(self, history: List, model=None, temperature=0.5, image_url: str = None) -> str:
        """Declare chat completion to be able to speak with the GPT and set desired temperature."""
        
        generationStartTime = datetime.now()
        # use model if provided, otherwise use self.model; if self.model is None, use gpt-4o-2024-05-13
        if model is None:
            if self.model is None:
                model = "gpt-4o-2024-05-13"
            else:
                model = self.model
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=history,
                temperature=temperature)
        except openai._exceptions.APIConnectionError as e:  # give one more try
            logger.warning(
                "API Connection Error. Waiting for {} seconds".format(
                    self.error_wait_time))
            
            logger.log("Connection Error: ", e)
            time.sleep(self.error_wait_time)
            response = openai.ChatCompletion.create(
                model=model,
                messages=history,
                temperature=temperature,)
        except openai._exceptions.RateLimitError as e:  # give one more try
            logger.warning("Rate limit reached. Waiting for 5 seconds")
            logger.error("Rate Limit Error: ", e)
            time.sleep(self.error_wait_time)
            print(f"Exception: {str(e)}")
            response = openai.ChatCompletion.create(
                model=model,
                messages=history,
                temperature=temperature,)
        except openai._exceptions.RateLimitError as e:  # token limit reached
            logger.warning("Token size limit reached. The recent message is compressed")
            logger.error("Token size error; will retry with compressed message ", e)
            # compress the message in two ways.
            ## 1. compress the last message
            history[-1]["content"] = self._token_compression(history)
            ## 2. reduce the number of messages in the history. Minimum is 2
            if self.history_length > 2:
                self.history_length -= 1
            ## update the history
            history = history[-self.history_length :]
            response = openai.ChatCompletion.create(
                model=model,
                messages=history,
                temperature=temperature,
            )

        # if the response is a tuple, it means that the response is not valid.
        if isinstance(response, tuple):
            logger.warning("Response is not valid. Waiting for 5 seconds")
            try:
                time.sleep(self.error_wait_time)
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=history,
                    temperature=temperature,)
                if isinstance(response, tuple):
                    logger.error("Response is not valid. ")
                    raise Exception("Response is not valid. ")
            except Exception as e:
                logger.error("Response is not valid. ", e)
                raise Exception(
                    "Response is not valid. The most likely reason is the connection to OpenAI is not stable. "
                    "Please doublecheck with `sspentestlab-connection`"
                )
        # add langfuse logging
        if hasattr(self, "langfuse"):
            generation = self.langfuse.generation(
                InitialGeneration(
                    name="chatgpt-completion",
                    startTime=generationStartTime,
                    endTime=datetime.now(),
                    model=self.model,
                    modelParameters={"temperature": str(temperature)},
                    prompt=history,
                    completion=response.choices[0].message.content,
                    usage=Usage(
                        promptTokens=response.usage.prompt_tokens,
                        completionTokens=response.usage.completion_tokens,
                    ),
                )
            )
        return response.choices[0].message.content