import dataclasses
import os
from typing import Any, Dict, List, Tuple
from rich.console import Console


import loguru
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage, Settings
from llama_index.core.node_parser import SentenceSplitter
from langchain_ollama import OllamaLLM
from llama_index.embeddings.ollama import OllamaEmbedding



Settings.llm = OllamaLLM(model="llama3.2:1b", temperature=0.8)              # -> model for interpreting embeddings

ollama_embedding = OllamaEmbedding(
    model_name="tazarov/all-minilm-l6-v2-f32:latest",                       # -> model for embeddings
)

Settings.embed_model = ollama_embedding
Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)

logger = loguru.logger
logger.remove()

class LlamaIndexRAG:
    def __init__(self, document_dir: str, persist_dir: str):
        """Initializes LlamaIndex with documents and sets up storage persistence."""
        
        self.document_dir = document_dir
        self.persist_dir = persist_dir
        self.doc_mapping = {}
        self.index = self._initialize_index()
        self.console = Console()


    def _initialize_index(self):
        """Checks if index exists, otherwise creates a new one."""
        
        if not os.path.exists(self.persist_dir):
            # if index does not exist, create a new one
            logger.info(f"Loading documents from: {self.document_dir}")
            documents = SimpleDirectoryReader(self.document_dir).load_data()

            # create a mapping of doc_id to file_name
            for doc in documents:
                self.doc_mapping[doc.doc_id] = doc.extra_info.get("file_name", "Archivo desconocido")

            index = VectorStoreIndex.from_documents(documents)
            index.storage_context.persist(persist_dir=self.persist_dir)
        else:
            # load index
            logger.info(f"Loading existing index from storage: {self.persist_dir}")
            storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
            index = load_index_from_storage(storage_context)
        return index

    def query_index(self, query: str):
        """Queries the LlamaIndex for relevant information."""
        
        query_engine = self.index.as_query_engine(response_mode="tree_summarize")
        response = query_engine.query(query)

        # extract relevant files
        relevant_files = set()
        for node_with_score in response.source_nodes:
            node = node_with_score.node

            # obtain file name from metadata
            try:
                file_name = node.metadata.get('file_name', "Archivo desconocido")
                relevant_files.add(file_name)
            except AttributeError:
                print("No metadata asociated to the node.")
                file_name = "Archivo desconocido"

        # debugging
            # print(f"Processed file: {file_name}")

        # show relevant files
        # if relevant_files:
        #     self.console.print(
        #         "\nResultados obtenidos de los siguientes archivos:", style="bold green")
        #     for file in relevant_files:
        #         self.console.print(f"- {file}", style="aquamarine1")
        # else:
        #     print("\nNo se encontraron archivos relevantes.")

        return str(response)